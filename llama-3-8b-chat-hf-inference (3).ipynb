{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from time import time\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom IPython.display import display, Markdown\n\nmodel_path = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ntext_pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\ndef generate_response(system_prompt, user_prompt, temp=0.7, max_len=1024):\n    start_time = time()\n    user_prompt = \"Question: \" + user_prompt + \" Answer:\"\n    prompts = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    prompt_text = text_pipeline.tokenizer.apply_chat_template(\n        prompts, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    eos_token_id = text_pipeline.tokenizer.eos_token_id\n\n    if eos_token_id is None:\n        raise ValueError(\"The tokenizer does not have an EOS token. Please check the tokenizer configuration.\")\n    \n    generated_sequences = text_pipeline(\n        prompt_text,\n        do_sample=True,\n        top_p=0.9,\n        temperature=temp,\n        eos_token_id=eos_token_id,\n        max_new_tokens=max_len,\n        return_full_text=False,\n        pad_token_id=text_pipeline.model.config.eos_token_id\n    )\n    response_text = generated_sequences[0]['generated_text']\n    \n    response_text = response_text.split(\"assistant\")[0].strip()\n    \n    end_time = time()\n    total_time = f\"Total time: {round(end_time - start_time, 2)} sec.\"\n\n    return user_prompt + \" \" + response_text + \" \" + total_time\n\n\ndef highlight_text(text):\n    for keyword, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n        text = text.replace(f\"{keyword}:\", f\"\\n\\n**<font color='{color}'>{keyword}:</font>**\")\n    return text\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T18:04:44.524095Z","iopub.execute_input":"2024-09-21T18:04:44.524866Z","iopub.status.idle":"2024-09-21T18:05:08.330732Z","shell.execute_reply.started":"2024-09-21T18:04:44.524811Z","shell.execute_reply":"2024-09-21T18:05:08.328582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n\n\n@app.route('/statistics_reply', methods=['POST'])\ndef process_one():\n    system_prompt = \"\"\"\n        You are a financial advisor focused on women empowerment. Your only response should be in HTML format, containing tables, charts, or lists  as needed. Do not provide explanations or add any extra text or commentary outside of the HTML structure.\n    \"\"\"    \n    data = request.json\n    prompt=data['prompt']\n    result = generate_response(\n    system_prompt,\n    user_prompt=prompt,\n    temp=0.1,\n    max_len=256\n    )\n    return jsonify({\"result\": result})\n\n@app.route('/get_reply', methods=['POST'])\ndef process_two():\n    system_prompt = \"\"\"\n        You are a financial advisor helping with women empowerment. Only respond with the HTML code of the chart, and do not include any additional text or commentary.\n    \"\"\"\n\n    data = request.json\n    prompt=data['prompt']\n    result = generate_response(\n    system_prompt,\n    user_prompt=prompt,\n    temp=0.1,\n    max_len=256\n    )\n    return jsonify({\"result\": result})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T18:05:08.332097Z","iopub.status.idle":"2024-09-21T18:05:08.332691Z","shell.execute_reply.started":"2024-09-21T18:05:08.332440Z","shell.execute_reply":"2024-09-21T18:05:08.332460Z"},"trusted":true},"execution_count":null,"outputs":[]}]}